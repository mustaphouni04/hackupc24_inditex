{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download a subset of the images provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(name, link, out_dir):\n",
    "\timg_name = os.path.join(out_dir, f\"{name}.jpg\")\n",
    "\tsubprocess.run([\"curl\", link, \"--output\", img_name], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "def download_images(df, n, out_dir=\"data/images\"):\n",
    "\tif not os.path.exists(out_dir):\n",
    "\t\tos.makedirs(out_dir)\n",
    "\twith concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tfor j, v in enumerate((\"IMAGE_VERSION_1\", \"IMAGE_VERSION_2\", \"IMAGE_VERSION_3\")):\n",
    "\t\t\t\tlink = df.iloc[i][v]\n",
    "\t\t\t\texecutor.submit(download_image, f\"{i}_{j}\", link, out_dir)\n",
    "            \n",
    "df = pd.read_csv(os.path.join(\"..\", \"data\", \"inditextech_hackupc_challenge_images.csv\"))\n",
    "download_images(df, 3000, os.path.join(\"..\", \"data\", \"images\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this may give error in some cases, resulting in corrupted files. This is solved by the following script, which removes those from the directory, and also those images that don'thave the right channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../data/images\"\n",
    "data = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "for path in data:\n",
    "\ttry:\n",
    "\t\timg = Image.open(path)\n",
    "\t\tassert np.array(img).shape[2] == 3\n",
    "\texcept:\n",
    "\t\tos.remove(path)\n",
    "\t\tprint(\"Removed corrupted image: \", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also make a quick dataloader to resize the images into a smaller, more managable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\tdef __init__(self, data):\n",
    "\t\tself.data = data\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg = Image.open(self.data[idx])\n",
    "\t\treturn img\n",
    "\n",
    "dataset = ImageDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resize the images to 256x256\n",
    "for i, img in tqdm(enumerate(dataloader)):\n",
    "\timg = img.to(device)\n",
    "\tresized_img = transforms.Resize((256, 256))(img)\n",
    "\tfor j in range(resized_img.size(0)):\n",
    "\t\ttorchvision.utils.save_image(resized_img[j], f\"../data/images/{i*32+j}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a Lazy Dataset, which doesn't initially contain all the images, but loads them one by one as needed.\n",
    "\n",
    "This Dataset will be used to fine-tune a ResNet neural net, which has been pretrained on ImageNet. As we have no labels, our finetuning is done in an original way: we randomly rotate the images and train the model to predict which has been the rotation. This gives the model understanding of our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyRotationImageDataset(Dataset):\n",
    "\tdef __init__(self, root_dir, transform=None):\n",
    "\t\tsuper(LazyRotationImageDataset, self).__init__()\n",
    "\t\tself.data = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith(\".jpg\")]\n",
    "\t\tself.rotations = [0, 90, 180, 270]\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_path = self.data[idx]\n",
    "\t\timage = Image.open(img_path)\n",
    "\n",
    "\t\tif self.transform:\n",
    "\t\t\timage = self.transform(image)\n",
    "\n",
    "\t\trotation_idx = torch.randint(0, 4, (1,)).item()  # Random index for rotation\n",
    "\t\trotation_angle = self.rotations[rotation_idx]  # Corresponding rotation angle\n",
    "\n",
    "\t\trotation_transform = transforms.Compose([\n",
    "\t\t\ttransforms.RandomRotation([rotation_angle, rotation_angle], expand=True),\n",
    "\t\t\ttransforms.ToTensor()\n",
    "\t\t])\n",
    "\n",
    "\t\trotated_image = rotation_transform(image)  # Applies the selected rotation\n",
    "\t\treturn rotated_image, rotation_idx\n",
    "\n",
    "dataset = LazyRotationImageDataset(\"../data/images\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model modification to predict rotation\n",
    "class RotationPredictor(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\t\tself.resnet.fc = nn.Linear(self.resnet.fc.in_features, 4)  # Predicting 4 rotation classes\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.resnet(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RotationPredictor().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally train the model here by your own, but we provide you with the model we've already trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\"\"\"\n",
    "model.train()\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "  for images, labels in tqdm(dataloader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model_finetunned = copy.deepcopy(model.resnet)\n",
    "model_finetunned = nn.Sequential(*list(model_finetunned.children())[:-4], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "model_finetunned = torch.quantization.quantize_dynamic(\n",
    "\tmodel_finetunned, {nn.Linear, nn.Conv2d}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/model_finetuned.pt\")\n",
    "\"\"\"\n",
    "\n",
    "resnet_original = models.resnet18(weights=None)\n",
    "model = nn.Sequential(*list(resnet_original.children())[:-4], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "model_weights_path = '../models/model_finetuned.pt'\n",
    "model.load_state_dict(torch.load(model_weights_path), strict=False)\n",
    "model.eval()\n",
    "\n",
    "# Now the model is fine-tuned to predict rotations, which also improves its feature extraction capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute features (embeddings), for the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features_finetuned = torch.empty((len(dataset), 128)).to(device)\n",
    "batch_size = 32\n",
    "\n",
    "for i, (images, labels) in tqdm(enumerate(dataloader)):\n",
    "\timages = images.to(device)\n",
    "\tlabels = labels.to(device)\n",
    "\toutputs = model_finetunned(images).squeeze().detach()\n",
    "\tfeatures_finetuned[i * batch_size: i * batch_size + len(images)] = outputs\n",
    "\n",
    "torch.save(features_finetuned, \"../data/sfeatures_finetuned.pt\")\n",
    "\"\"\"\n",
    "features_finetuned = torch.load(\"../data/features_finetuned.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use this extracted features to compare images by similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_names(directory):\n",
    "    image_extensions = '.jpg'\n",
    "    image_names = []\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if any(filename.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_names.append(filename)\n",
    "    return image_names\n",
    "\n",
    "images_dir = \"../data/images/resized\"\n",
    "image_names = get_image_names(images_dir)\n",
    "\n",
    "finetunning_matrix = []\n",
    "for image, vector in zip(image_names, features_finetuned):\n",
    "    finetunning_matrix.append([image, vector])\n",
    "finetunning_matrix = np.array(finetunning_matrix, dtype=object)\n",
    "\n",
    "def top_k_similar_images(embeddings_with_names, specific_image_name, k=10):\n",
    "    specific_image_index = None\n",
    "    for i, row in enumerate(embeddings_with_names):\n",
    "        if row[0] == specific_image_name:\n",
    "            specific_image_index = i\n",
    "            break\n",
    "\n",
    "    if specific_image_index is None:\n",
    "        raise ValueError(\"The specific image name was not found in the data matrix.\")\n",
    "    specific_embedding = embeddings_with_names[specific_image_index][1].reshape(1, -1)\n",
    "    all_embeddings = np.array([row[1] for row in embeddings_with_names])\n",
    "    similarities = cosine_similarity(specific_embedding, all_embeddings)\n",
    "    similar_image_indices = np.argsort(-similarities)[0][:k]\n",
    "    similar_image_links = embeddings_with_names[similar_image_indices, 0]\n",
    "\n",
    "    return similar_image_links.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a simple demo to try it. An image has to be selected from the directory of images the model the features were extracted from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_show(img_path):\n",
    "\timg_name = os.path.basename(img_path).replace(\".jpeg\", \".jpg\")\n",
    "\timages_name = top_k_similar_images(finetunning_matrix, img_name, k=6)\n",
    "\treturn [os.path.join(images_dir, images_name[i]) for i in range(6)]\n",
    "\n",
    "def set_as_input(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    blank = np.ones_like(img)*255\n",
    "    return img_path, blank, blank, blank, blank, blank, blank\n",
    "\n",
    "with gr.Blocks() as gui:\n",
    "\twith gr.Column():\n",
    "\t\twith gr.Row():\n",
    "\t\t\twith gr.Column():\n",
    "\t\t\t\timg_in = gr.Image(type=\"filepath\")\n",
    "\t\t\t\tbtn = gr.Button(\"Search\")\n",
    "\t\twith gr.Row():\n",
    "\t\t\twith gr.Column():\n",
    "\t\t\t\timg_out1 = gr.Image(show_download_button=False, interactive=False, type=\"filepath\")\n",
    "\t\t\t\tbtn1 = gr.Button(\"Set as input\")\n",
    "\t\t\twith gr.Column():\n",
    "\t\t\t\timg_out2 = gr.Image(show_download_button=False, interactive=False, type=\"filepath\")\n",
    "\t\t\t\tbtn2 = gr.Button(\"Set as input\")\n",
    "\t\t\twith gr.Column():\n",
    "\t\t\t\timg_out3 = gr.Image(show_download_button=False, interactive=False, type=\"filepath\")\n",
    "\t\t\t\tbtn3 = gr.Button(\"Set as input\")\n",
    "\t\twith gr.Row():\n",
    "\t\t\twith gr.Column():\n",
    "\t\t\t\timg_out4 = gr.Image(show_download_button=False, interactive=False, type=\"filepath\")\n",
    "\t\t\t\tbtn4 = gr.Button(\"Set as input\")\n",
    "\t\t\twith gr.Column():\n",
    "\t\t\t\timg_out5 = gr.Image(show_download_button=False, interactive=False, type=\"filepath\")\n",
    "\t\t\t\tbtn5 = gr.Button(\"Set as input\")\n",
    "\t\t\twith gr.Column():\n",
    "\t\t\t\timg_out6 = gr.Image(show_download_button=False, interactive=False, type=\"filepath\")\n",
    "\t\t\t\tbtn6 = gr.Button(\"Set as input\")\n",
    "  \n",
    "\tbtn.click(images_to_show, inputs=img_in, outputs=[img_out1, img_out2, img_out3, img_out4, img_out5, img_out6])\n",
    "\tbtn1.click(set_as_input, inputs=img_out1, outputs=[img_in, img_out1, img_out2, img_out3, img_out4, img_out5, img_out6])\n",
    "\tbtn2.click(set_as_input, inputs=img_out2, outputs=[img_in, img_out1, img_out2, img_out3, img_out4, img_out5, img_out6])\n",
    "\tbtn3.click(set_as_input, inputs=img_out3, outputs=[img_in, img_out1, img_out2, img_out3, img_out4, img_out5, img_out6])\n",
    "\tbtn4.click(set_as_input, inputs=img_out4, outputs=[img_in, img_out1, img_out2, img_out3, img_out4, img_out5, img_out6])\n",
    "\tbtn5.click(set_as_input, inputs=img_out5, outputs=[img_in, img_out1, img_out2, img_out3, img_out4, img_out5, img_out6])\n",
    "\tbtn6.click(set_as_input, inputs=img_out6, outputs=[img_in, img_out1, img_out2, img_out3, img_out4, img_out5, img_out6])\n",
    "\n",
    "gui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Possible Improvements on efficiency: Clustering\n",
    "\n",
    "If our dataset of clothes becomes to big, a lot of computational power might be used inecesssarily as data points that are very dissimilar are treated and compared equally like similar points.  \n",
    "We tought of grouping the data into clusters, and only check for similarites on members of that cluster for a given point.  \n",
    "We currently didn't include it in the final result (altought we implemented it), as the performance slightly dropped and, with a small dataset, the increase in efficiency was small. But we believe that for much bigger dataset (millions of images), this approach will be a necesity.\n",
    "\n",
    "This approach is developped in more depth in the `clustering.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
